Code repository for LMSRR (Learning Multi-Source and Robust Representations for Continual Learning). The code will be made available upon publication.
=======
# ğŸš€ LMSRR

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/) [![Paper](https://img.shields.io/badge/Paper-OpenReview-red)](https://openreview.net/pdf?id=24vq7c6MpR)![License](https://img.shields.io/badge/License-MIT-green.svg) ![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-purple.svg)

Code for paper **"[Learning Multi-Source and Robust Representations for Continual Learning](https://openreview.net/pdf?id=24vq7c6MpR)"** NeurIPS2025.

------

## ğŸ“Œ Table of Contents

- [Introduction](#ğŸ” Introduction)
- [Framework Overview](#ğŸ§  Framework Overview)
- [Key Contributions](#ğŸŒŸ Key Contributions)
- [Installation](#âš™ï¸ Installation)
- [Usage](#â–¶ï¸ Usage)
- [Citation](#ğŸ“ Citation)
- [Acknowledgement](#ğŸ™ Acknowledgement)

------

## ğŸ” Introduction

Continual learning models must strike a delicate balance between **plasticity** (learning new tasks effectively) and **stability** (retaining previous knowledge). Although many recent methods utilize pre-trained backbones to improve stability, they largely rely on **a single backbone**, limiting adaptiveness and representation richness.

**LMSRR** introduces a **multi-source, dynamically optimized representation framework**, combining multiple heterogeneous pre-trained models with a novel set of optimization strategies, yielding robust and adaptive features for continual learning.

------

## ğŸ§  Framework Overview

<p align="center">
  <img src="fig/framework.png" width="60%">
</p>

LMSRR contains three major components:

### **1. Multi-Scale Interaction & Dynamic Fusion (MSIDF)**

- Interacts multi-source features across scales.
- Learns task-relevant feature selection via attention modules.

### **2. Multi-Level Representation Optimization (MLRO)**

- Dynamically refines backbone layers.
- Improves plasticity while preserving critical representations.

### **3. Adaptive Regularization Optimization (ARO)**

- Learns a switch vector controlling layerwise updating.
- Avoids over-regularization and improves new task learning.

Together, these form a unified optimization framework offering a strong trade-off between **stability** and **plasticity**.

------

## ğŸŒŸ Key Contributions

- **Multi-source representation learning** via coordinated pre-trained backbones.
- **Dynamic multi-scale fusion** (MSIDF) capturing cross-source semantic complementarities.
- **Adaptive multi-level optimization** (MLRO) improving plasticity.
- **Layerwise adaptive regularization** (ARO) preventing catastrophic forgetting.
- **State-of-the-art performance** across standard continual learning benchmarks.

------

## âš™ï¸ Installation

```bash
conda create -n LMSRR4CL python=3.10
conda activate LMSRR4CL
pip install -r requirements.txt
```

------

## â–¶ï¸ Usage

### **1. Enter the project directory**

```bash
cd LMSRR/
```

### **2. Run the example training script**

```bash
bash command/lmsrr_cifar10.sh
bash command/lmsrr_cifar100.sh
```

### Project structure overview

```bash
LMSRR/
â”‚â”€â”€ backbone/                # Pre-trained backbone models
â”‚   â”œâ”€â”€ LMSRR.py			 # LMSRR backbone implementation
â”‚   â”œâ”€â”€ ...
â”‚â”€â”€ command/                 # Training scripts
â”‚â”€â”€ datasets/                # Dataset loaders
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                   # Method implementations
â”‚   â”œâ”€â”€ lmsrr.py              # LMSRR method implementation
â”‚   â””â”€â”€ <baseline>.py         # Other baseline methods (e.g., ER, DER++, etc.)
â”‚â”€â”€ utils/                   # Helper tools
â”‚   â””â”€â”€ ...
â”‚â”€â”€ main.py                  # Main training entry point
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md                # Project documentation
```

------

## ğŸ“ Citation

If you find this repository helpful, please cite our paper:

```
@inproceedings{ye2025lmsrr,
  title={Learning Multi-Source and Robust Representations for Continual Learning},
  author={Ye, Fei and Zhong, Yongcheng and Liu, Qihe and Bors, Adrian G and Hu, Rongyao and others},
  booktitle={Proceedings of the 39th Conference on Neural Information Processing Systems},
  year={2025}
}
```

------

## ğŸ™ Acknowledgement

This project is built upon the excellent continual learning framework **[Mammoth](https://github.com/aimagelab/mammoth)**.
 We sincerely thank the authors for open-sourcing their work.

>>>>>>> 877ff29 (chore: local initial commit)
